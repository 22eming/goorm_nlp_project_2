{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Korean MRC Baseline\n","\n","## Dependency\n","다음과 같은 라이브러리를 사용한다.\n","- [Konlpy](https://konlpy.org/ko/latest/index.html): 파이썬 한국어 NLP 처리기\n","- [Mecab-korean](https://bitbucket.org/eunjeon/mecab-ko-dic/src): 한국어 형태소 분석기"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["! apt-get install -y openjdk-8-jdk python3-dev\n","! pip install konlpy \"tweepy<4.0.0\"\n","! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Git clone\n","!git clone https://github.com/22eming/goorm_nlp_project_2.git\n","%cd /content/goorm_nlp_project_2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'konlpy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/sawol22/Desktop/ai/project2/korean-mrc-baseline-goorm-4.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sawol22/Desktop/ai/project2/korean-mrc-baseline-goorm-4.ipynb#ch0000038?line=0'>1</a>\u001b[0m \u001b[39m# from typing import List, Tuple, Dict, Any, Generator\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sawol22/Desktop/ai/project2/korean-mrc-baseline-goorm-4.ipynb#ch0000038?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkonlpy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sawol22/Desktop/ai/project2/korean-mrc-baseline-goorm-4.ipynb#ch0000038?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minit_KoMRC\u001b[39;00m \u001b[39mimport\u001b[39;00m  KoMRC\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sawol22/Desktop/ai/project2/korean-mrc-baseline-goorm-4.ipynb#ch0000038?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtoken_KoMRC\u001b[39;00m \u001b[39mimport\u001b[39;00m token_KoMRC\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"]}],"source":["from typing import List, Tuple, Dict, Any, Generator\n","import json\n","import random\n","import konlpy\n","\n","from init_KoMRC import KoMRC\n","from token_KoMRC import TokenizedKoMRC\n","from init_indexer import Indexer, IndexerWrappedDataset\n","from init_model import BertForQuestionAnswering\n","from init_collator import Collator"]},{"cell_type":"markdown","metadata":{},"source":["## 데이터셋 구성"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = KoMRC.load('data/train.json')\n","train_dataset, dev_dataset = KoMRC.split(dataset) # split(dataset, 0.2) => 8:2 split"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 단어 단위 토큰화\n","dataset = TokenizedKoMRC.load('data/train.json')\n","train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Vocab 생성 및 Indexing\n","토큰화된 데이터 셋을 기준으로 Vocab을 만들고 인덱싱을 하는 `Indexer`를 만들자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 토큰화 데이터셋 기준 vocab만들고 인덱싱\n","indexer = Indexer.build_vocab(dataset)\n","print(indexer.sample2ids(dev_dataset[0]))"]},{"cell_type":"markdown","metadata":{},"source":["쉽게 Indexer를 활용하기 위해 Indexer가 포함된 데이터 셋을 만들자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\n","indexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)\n","\n","sample = indexed_dev_dataset[0]\n","print(sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], sample['start'], sample['end'])"]},{"cell_type":"markdown","metadata":{},"source":["## 학습 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 64\n","accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자\n","\n","collator = Collator(indexer)\n","train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)\n","dev_loader = DataLoader(indexed_dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch = next(iter(dev_loader))\n","print(batch['input_ids'].shape)\n","print(batch['input_ids'])\n","print(list(batch.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import BertConfig\n","\n","torch.manual_seed(42)\n","config = BertConfig(\n","     vocab_size=indexer.vocab_size,\n","     max_position_embeddings=1024,\n","     hidden_size=256,\n","     num_hidden_layers=4,\n","     num_attention_heads=4,\n","     intermediate_size=1024\n",")\n","model = BertForQuestionAnswering(config)\n","model.cuda()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","from statistics import mean\n","\n","import torch.nn.functional as F\n","from torch.nn.utils import clip_grad_norm_\n","\n","os.makedirs('dump', exist_ok=True)\n","train_losses = []\n","dev_losses = []\n","\n","step = 0\n","\n","for epoch in range(1, 31):\n","    print(\"Epoch\", epoch)\n","    # Training\n","    running_loss = 0.\n","    losses = []\n","    progress_bar = tqdm(train_loader, desc='Train')\n","    for batch in progress_bar:\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        # batch = {key: value.cuda() for key, value in batch.items()}\n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        start_logits, end_logits = model(**batch)\n","        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","        (loss / accumulation).backward()\n","        running_loss += loss.item()\n","        del batch, start, end, start_logits, end_logits, loss\n","        \n","        step += 1\n","        if step % accumulation:\n","            continue\n","\n","        clip_grad_norm_(model.parameters(), max_norm=1.)\n","        optimizer.step()\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        losses.append(running_loss / accumulation)\n","        running_loss = 0.\n","        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n","    train_losses.append(mean(losses))\n","    print(f\"train score: {train_losses[-1]:.3f}\")\n","\n","    # Evaluation\n","    losses = []\n","    for batch in tqdm(dev_loader, desc=\"Evaluation\"):\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        # batch = {key: value.cuda() for key, value in batch.items()}\n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        with torch.no_grad():\n","            start_logits, end_logits = model(**batch)\n","        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","\n","        losses.append(loss.item())\n","        del batch, start, end, start_logits, end_logits, loss\n","    dev_losses.append(mean(losses))\n","    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n","\n","    model.save_pretrained(f'dump/model.{epoch}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","t = list(range(1, 31))\n","plt.plot(t, train_losses, label=\"Train Loss\")\n","plt.plot(t, dev_losses, label=\"Dev Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["![loss_plot](https://github.com/mynsng/mynsng.github.io/blob/master/assets/images/__results___26_0.png?raw=true)"]},{"cell_type":"markdown","metadata":{},"source":["학습 데이터 셋에 Overfitting이 일어나는 것을 확인할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["## Answer Inference\n","모델의 Output을 활용해서 질문의 답을 찾는 코드를 작성하자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = BertForQuestionAnswering.from_pretrained('dump/model.30')\n","model.cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for idx, sample in zip(range(1, 4), indexed_train_dataset):\n","    print(f'------{idx}------')\n","    print('Context:', sample['context'])\n","    print('Question:', sample['question'])\n","    \n","    input_ids, token_type_ids = [\n","        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","        for key in (\"input_ids\", \"token_type_ids\")\n","    ]\n","    \n","    with torch.no_grad():\n","        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","    start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","    index = torch.argmax(probability).item()\n","    \n","    start = index // len(end_prob)\n","    end = index % len(end_prob)\n","    \n","    start = sample['position'][start][0]\n","    end = sample['position'][end][1]\n","\n","    print('Answer:', sample['context'][start:end])"]},{"cell_type":"markdown","metadata":{},"source":["## Test 출력 파일 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-4-korean-mrc/test.json')\n","test_dataset = IndexerWrappedDataset(test_dataset, indexer)\n","print(\"Number of Test Samples\", len(test_dataset))\n","print(test_dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import csv\n","\n","os.makedirs('out', exist_ok=True)\n","with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n","    writer = csv.writer(fd)\n","    writer.writerow(['Id', 'Predicted'])\n","\n","    rows = []\n","    for sample in tqdm(test_dataset, \"Testing\"):\n","        input_ids, token_type_ids = [\n","            torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","            for key in (\"input_ids\", \"token_type_ids\")\n","        ]\n","    \n","        with torch.no_grad():\n","            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","        start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","        index = torch.argmax(probability).item()\n","    \n","        start = index // len(end_prob)\n","        end = index % len(end_prob)\n","    \n","        start = sample['position'][start][0]\n","        end = sample['position'][end][1]\n","\n","        rows.append([sample[\"guid\"], sample['context'][start:end]])\n","    \n","    writer.writerows(rows)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
